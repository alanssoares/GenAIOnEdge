version: '3.8'

services:
  # Frontend service
  frontend:
    build:
      context: ./front/chat-frontend
      dockerfile: Dockerfile
    ports:
      - "3000:80"
    depends_on:
      - proxy
    networks:
      - genai-network

  # Proxy service
  proxy:
    build:
      context: ./proxy
      dockerfile: Dockerfile
    ports:
      - "8000:8000"
    depends_on:
      - llama
      - gpt-neo
      - mistral
    networks:
      - genai-network

  # Llama 2 model service
  llama:
    build:
      context: ./models/models/llama2/fastAPI
      dockerfile: Dockerfile
    ports:
      - "8001:8000"
    volumes:
      - ./models/models/llama2/model:/app/model
    environment:
      - MODEL_PATH=/app/model
    networks:
      - genai-network
    deploy:
      resources:
        limits:
          memory: 8G
        reservations:
          memory: 4G

  # GPT-Neo model service
  gpt-neo:
    build:
      context: ./models/models/gpt_neo/fastAPI
      dockerfile: Dockerfile
    ports:
      - "8002:8000"
    volumes:
      - ./models/models/gpt_neo/model:/app/model
    environment:
      - MODEL_PATH=/app/model
    networks:
      - genai-network
    deploy:
      resources:
        limits:
          memory: 8G
        reservations:
          memory: 4G

  # Mistral model service
  mistral:
    build:
      context: ./models/models/mistral/fastAPI
      dockerfile: Dockerfile
    ports:
      - "8003:8000"
    volumes:
      - ./models/models/mistral/model:/app/model
    environment:
      - MODEL_PATH=/app/model
    networks:
      - genai-network
    deploy:
      resources:
        limits:
          memory: 8G
        reservations:
          memory: 4G

networks:
  genai-network:
    driver: bridge

volumes:
  llama_models:
  gpt_neo_models:
  mistral_models: